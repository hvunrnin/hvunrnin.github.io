---
title: "서버 메모리 해결"
author: "hvunrnin"
date: 2025-07-15 10:00:00 
categories: [Server, Memory]
tags: [Grafana, Monitoring]
math: true
toc: true
pin: true
---

# Spring 서버 메모리 부족 문제, Grafana 모니터링으로 해결하기

## 문제 상황

토이 프로젝트로 개발한 Spring 애플리케이션을 EC2에 배포했는데, 계속해서 이상한 문제가 발생했다.

### 증상
- 평소에는 정상 작동
- **조금만 부하를 주면 서버가 다운**
- 재시작하면 다시 정상 작동

### 초기 추측
처음에는 CPU 문제인 줄 알았다. 하지만 팀원에게 물어보니 "CPU는 문제없고, 메모리 문제 같다"는 답변을 받았다. 그리고 임시 해결책으로 **Swap 메모리** 설정을 하기로 했다.

하지만 단순히 해결책만 적용하기보다는, 정확한 원인을 파악하고 싶어서 모니터링부터 시작했다.

## 문제 분석

### Grafana 모니터링 도입

먼저 현재 서버 상태를 정확히 파악하기 위해 Grafana를 통한 모니터링을 구축했다.

<img src="assets/img/SWAP/image.png"  alt="1" width="800"/>

### 데이터 분석 결과

모니터링 결과를 분석해보니 명확한 패턴이 보였다:

**✅ CPU 사용률**
- 대부분 4% 이하로 매우 낮음
- CPU는 확실히 병목이 아니었음

**⚠️ 메모리 사용 패턴**
- **주기적으로 발생하는 메모리 스파이크**
- 15-20MB 구간에서 반복적인 피크 발생
- 불안정한 메모리 사용량 변동

**🔍 컨테이너별 메모리 사용량**
```
- kafka-0: 20.4MB (최대 사용량)
- mongodb: 상당한 메모리 점유
- grafana, chat-producer 등 다수 컨테이너 동시 실행
- 총 15개 컨테이너가 메모리 경합
```

**📊 Memory Cached 변동**
- 지속적으로 변동하는 캐시 사용량
- 시스템이 메모리 압박 상태임을 시사

### 문제 원인 파악

분석 결과, 문제의 핵심은:
1. **여러 컨테이너가 동시에 메모리를 사용**하는 환경
2. **예측 불가능한 메모리 스파이크** 발생
3. **물리 메모리 한계**에 도달했을 때 OOM Kill 발생

## 💡 해결 방안 선택

### 검토한 해결책들

**1. EC2 인스턴스 업그레이드**
- ✅ 근본적 해결
- ❌ 비용 부담 (토이 프로젝트에는 과도)

**2. 애플리케이션 최적화**
- ✅ 장기적으로 필요
- ❌ 시간 소요, 즉시 해결 어려움

**3. Swap 메모리 설정**
- ✅ 즉시 적용 가능
- ✅ 비용 부담 없음
- ✅ OOM Kill 방지 효과
- ❌ 성능 저하 (디스크 I/O)

### Swap 메모리 선택 이유

토이 프로젝트 환경에서는 **비용 효율성**이 가장 중요했다. Swap은 성능 저하라는 단점이 있지만, 서버 다운보다는 훨씬 나은 선택이었다.

## 구현 과정

### 1. 현재 메모리 상태 확인

```bash
# 메모리 사용량 확인
free -h

# Swap 상태 확인 (설정 전에는 0으로 표시)
swapon --show
```

### 2. Swap 파일 생성

```bash
# 2GB Swap 파일 생성 (물리 메모리의 약 2배)
sudo fallocate -l 2G /swapfile

# 보안을 위한 파일 권한 설정
sudo chmod 600 /swapfile
```

### 3. Swap 설정 및 활성화

```bash
# Swap 포맷으로 변경
sudo mkswap /swapfile

# Swap 활성화
sudo swapon /swapfile
```

### 4. 영구 설정 (재부팅 후에도 유지)

```bash
# /etc/fstab에 추가하여 부팅 시 자동 마운트
echo '/swapfile swap swap defaults 0 0' | sudo tee -a /etc/fstab
```

### 5. Swap 최적화

```bash
# swappiness 값 조정 (기본값 60을 10으로 낮춤)
# 값이 낮을수록 물리 메모리를 우선 사용
echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf
```

### 6. 설정 검증

```bash
# 설정 결과 확인
free -h
```

예상 출력:
```
              total        used        free      shared  buff/cache   available
Mem:         1.0Gi       600Mi       100Mi          0B       300Mi       400Mi
Swap:        2.0Gi         0B       2.0Gi
```

## 결과 및 개선 효과

### Before vs After

**적용 전:**
- 부하 시 서버 다운 (OOM Kill)
- 불안정한 서비스 운영
- 예측 불가능한 장애

**적용 후:**
- 메모리 부족 시 Swap 활용으로 서버 유지
- 안정적인 서비스 운영
- 성능은 다소 저하되었지만 가용성 확보

### 성능 영향 분석

- **긍정적 효과**: 서버 다운 현상 완전 해결
- **부정적 영과**: 메모리 스와핑 시 응답 속도 약간 증가
- **전체 평가**: 토이 프로젝트 환경에서는 충분히 만족스러운 결과

## 회고 및 한계점

### 배운 점

1. **데이터 기반 문제 해결의 중요성**
   - 추측보다는 모니터링을 통한 정확한 현상 파악
   - Grafana를 통한 시각적 데이터 분석의 효과

2. **리소스 제약 환경에서의 최적화 경험**
   - 비용과 성능 사이의 트레이드오프 고려
   - 상황에 맞는 실용적 해결책 선택

3. **시스템 운영 지식 습득**
   - Linux 메모리 관리 이해
   - Swap 메모리의 동작 원리와 활용법

### 한계점 및 향후 개선 방향

**현재 해결책의 한계:**
- 근본적인 메모리 최적화는 이루어지지 않음

**향후 개선 계획:**
1. **애플리케이션 레벨 최적화**
   - JVM 힙 메모리 튜닝
   - 메모리 리크 가능성 점검
   - 커넥션 풀 크기 최적화

2. **인프라 확장 고려**
   - 프로젝트 규모 확대 시 인스턴스 업그레이드
   - 컨테이너 리소스 제한 설정

3. **지속적인 모니터링**
   - Swap 사용량 추적
   - 성능 메트릭 지속 관찰


---

토이 프로젝트에서 발생한 메모리 부족 문제를 해결하면서, 단순한 해결책 적용을 넘어서 **체계적인 문제 분석과 해결 과정**을 경험할 수 있었다.

비록 Swap 메모리는 임시방편적 해결책이지만, 제한된 리소스 환경에서 서비스 안정성을 확보하는 실용적인 방법임을 확인했다. 더 중요한 것은 **모니터링을 통한 데이터 기반 의사결정**의 중요성을 깨달았다.
